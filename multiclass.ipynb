{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUjHnfBTltA0",
        "outputId": "877ab088-2c75-49a6-c6d4-694e6c581f57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated W1:\n",
            " [[0.19932867 0.39994074 0.1021325 ]\n",
            " [0.49949651 0.29995555 0.20159938]\n",
            " [0.29941259 0.69994815 0.80186594]]\n",
            "Updated B1:\n",
            " [0.09916084 0.19992592 0.30266563]\n",
            "Updated W2:\n",
            " [[0.59803629 0.40449781 0.49746591]\n",
            " [0.09696517 0.20695115 0.29608367]\n",
            " [0.29729671 0.70619178 0.19651151]]\n",
            "Updated B2:\n",
            " [0.09744973 0.20584131 0.29670897]\n",
            "Output after softmax: [0.25502746 0.41586939 0.32910315]\n",
            "Gradient of loss with respect to output layer activations: [ 0.25502746 -0.58413061  0.32910315]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Input values (X)\n",
        "X = np.array([0.8, 0.6, 0.7])\n",
        "\n",
        "# Weights between input and hidden layer (W1)\n",
        "W1 = np.array([[0.2, 0.4, 0.1],\n",
        "               [0.5, 0.3, 0.2],\n",
        "               [0.3, 0.7, 0.8]])\n",
        "\n",
        "# Biases for hidden layer (B1)\n",
        "B1 = np.array([0.1, 0.2, 0.3])\n",
        "\n",
        "# Weights between hidden and output layer (W2)\n",
        "W2 = np.array([[0.6, 0.4, 0.5],\n",
        "               [0.1, 0.2, 0.3],\n",
        "               [0.3, 0.7, 0.2]])\n",
        "\n",
        "# Biases for output layer (B2)\n",
        "B2 = np.array([0.1, 0.2, 0.3])\n",
        "\n",
        "# True labels (Y)\n",
        "Y = np.array([0, 1, 0])\n",
        "\n",
        "# Activation Functions\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return np.where(x > 0, 1, 0)\n",
        "\n",
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x))  # stability improvement\n",
        "    return e_x / e_x.sum(axis=0)  # ensure softmax is applied along the correct axis\n",
        "\n",
        "# Forward Pass\n",
        "\n",
        "# Hidden layer activations\n",
        "H_pre_activation = np.dot(X, W1) + B1\n",
        "H = relu(H_pre_activation)\n",
        "\n",
        "# Output layer activations\n",
        "O_pre_activation = np.dot(H, W2) + B2\n",
        "O = softmax(O_pre_activation)\n",
        "\n",
        "# Backpropagation\n",
        "\n",
        "# Step 1: Calculate gradients of the loss with respect to output layer activations\n",
        "dLoss_dO = O - Y  # Gradient of loss w.r.t. output\n",
        "\n",
        "# Step 2: Calculate gradients of the loss with respect to weights between hidden and output layer (W2)\n",
        "dO_dW2 = np.outer(H, dLoss_dO)  # Gradient of the output layer w.r.t. W2\n",
        "dO_dB2 = dLoss_dO  # Gradient w.r.t. biases (B2)\n",
        "\n",
        "# Step 3: Propagate the error back to the hidden layer\n",
        "dO_dH = np.dot(W2, dLoss_dO)  # Gradient of output w.r.t. hidden layer\n",
        "dH_pre_activation = dO_dH * relu_derivative(H_pre_activation)  # Applying ReLU derivative to backpropagate through ReLU\n",
        "\n",
        "# Step 4: Calculate gradients of the loss w.r.t. weights between input and hidden layer (W1)\n",
        "dH_dW1 = np.outer(X, dH_pre_activation)  # Gradient of hidden layer w.r.t. W1\n",
        "dH_dB1 = dH_pre_activation  # Gradient w.r.t. biases (B1)\n",
        "\n",
        "# Learning rate\n",
        "learning_rate = 0.01\n",
        "\n",
        "# Step 5: Update weights and biases using gradient descent\n",
        "W2 -= learning_rate * dO_dW2\n",
        "B2 -= learning_rate * dO_dB2\n",
        "\n",
        "W1 -= learning_rate * dH_dW1\n",
        "B1 -= learning_rate * dH_dB1\n",
        "\n",
        "# Print updated weights and biases\n",
        "print(\"Updated W1:\\n\", W1)\n",
        "print(\"Updated B1:\\n\", B1)\n",
        "print(\"Updated W2:\\n\", W2)\n",
        "print(\"Updated B2:\\n\", B2)\n",
        "\n",
        "# Print results\n",
        "print(\"Output after softmax:\", O)\n",
        "print(\"Gradient of loss with respect to output layer activations:\", dLoss_dO)\n"
      ]
    }
  ]
}